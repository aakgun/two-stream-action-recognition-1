{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYRff_wiRRlF",
    "colab_type": "text"
   },
   "source": [
    "# Get colab access file (Run this cell for every google account)\n",
    "This will download 'cred_this_is_me.txt' which is your credentials file to access your drive from drive api / the code will access your drive folders/files to save and load models trained on colab\n",
    "\n",
    "Look at the **LICENSE**  and don't use a google account with any critical information (the credentials file will be downloaded locally so keep it safe) \n",
    "\n",
    "You will need to run this cell for each drive account you have(look at the account you are using from top left of the page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JBvzBNJMRVZM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# SECUIRTY DISCLAIMER: please look at the LICENCE file given with the code\n",
    "# make sure your are signed up with the google account you need to use with this code (the code can read/write data to your drive)\n",
    "# --------------------------------------------------------------------------------------------------------------------------------\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from google.colab import files\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "gauth.SaveCredentialsFile('cred_this_is_me.txt')  # we need this to start with cred*\n",
    "files.download('cred_this_is_me.txt')  # download and keep it safe you will need it later to identify your account\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC3m9tuOiXqX",
    "colab_type": "text"
   },
   "source": [
    "##Important :\n",
    "\n",
    "As the code uses google drive for storage you need some folder with write access,\n",
    "\n",
    "1.   So after the credentials are downloaded **cred_this_is_me.txt **  to your machine \n",
    "2.   Go to the class ./utils/drive_manager.py in the code and modify the **personal_dfolder** with your folder similar to my folder \"1B82anWV8Mb4iHYmOp9tIR9aOTlfllwsD\"\n",
    "\n",
    "3.   Copy **cred_this_is_me.txt ** to your ./utils/\n",
    "4.   use the upload.sh script that uses transfer.sh and you will have the link of the modified code can be downloaded here locally on Colab in the next steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zcjJG-MjwRw",
    "colab_type": "text"
   },
   "source": [
    "# Installations and system checking\n",
    "\n",
    "We need at least 100GB of storage where some colab machines are only 40 GB of storage which won't be enough. \n",
    "\n",
    "**Please clone this notebook, don't copy the cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "l9sxYJictJwW",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!df ~ --block-size=G\n",
    "#!du --block-size=M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CvSGJlQOwRff",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install tqdm 2> s.txt >> s.txt\n",
    "#pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl  2> s.txt >> s.txt\n",
    "#pip3 install torchvision 2> s.txt >> s.txt\n",
    "pip install -U -q PyDrive 2> s.txt >> s.txt\n",
    "pip install opencv-python 2> s.txt >> s.txt\n",
    "#https://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html\n",
    "mount -o remount,size=8G /dev/shm \n",
    "pip install imgaug 2> s.txt >> s.txt\n",
    "\n",
    "# THE REPO .. you are supposed to replace the code below using transfer.sh \n",
    "git clone https://github.com/mohammed-elkomy/two-stream-action-recognition.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "yJR9jr2z53I8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/content/two-stream-action-recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27f8QtTK6aDg",
    "colab_type": "text"
   },
   "source": [
    "# Download & unzip : Processed data as frames (RGB)\n",
    "Here I download the processed frames .. They are zipped into one file splitted into three parts..\n",
    "\n",
    "when unziped it should be 33GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jw3JGI-jyrtC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# at /content/two-stream-action-recognition\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.001 &\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.002 &\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.003 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ckvFVcs00AVZ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# at /content/two-stream-action-recognition\n",
    "cat ucf101_jpegs_256.zip.00* > ucf101_jpegs_256.zip &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "AI3KD-nJfRfV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# at /content/two-stream-action-recognition\n",
    "!rm ucf101_jpegs_256.zip.00*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-4zTCzi7aLxV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# at /content/two-stream-action-recognition\n",
    "unzip ucf101_jpegs_256.zip >> zip1.out &\n",
    "\n",
    "# data path has this format\n",
    "#v_ApplyEyeMakeup_g01_c01      v_Knitting_g16_c03\n",
    "#v_ApplyEyeMakeup_g01_c02      v_Knitting_g16_c04\n",
    "#v_ApplyEyeMakeup_g01_c03      v_Knitting_g16_c05\n",
    "#v_ApplyEyeMakeup_g01_c04      v_Knitting_g17_c01\n",
    "\n",
    "# each video folder has this format\n",
    "#frame000001.jpg  frame000047.jpg  frame000093.jpg  frame000139.jpg\n",
    "#frame000002.jpg  frame000048.jpg  frame000094.jpg  frame000140.jpg\n",
    "#frame000003.jpg  frame000049.jpg  frame000095.jpg  frame000141.jpg\n",
    "#frame000004.jpg  frame000050.jpg  frame000096.jpg  frame000142.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GHdtZo8h3B2g",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# at /content/two-stream-action-recognition\n",
    "!rm ucf101_jpegs_256.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ni-HTfOK3Zyd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# view the data jpegs_256 should be 33G\n",
    "!du -sh --human-readable *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRGtrOssijlN",
    "colab_type": "text"
   },
   "source": [
    "# Download & unzip : Processed data as frames (Flow)\n",
    "Here I download the processed u/v optical flow frames .. \n",
    "\n",
    "They are zipped into one file splitted into three parts.. when unziped it should be 31GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "j7VTeougjC7j",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# at /content/two-stream-action-recognition\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.001 &\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.002 &\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.003 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GZNT0D7CjDC9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# at /content/two-stream-action-recognition\n",
    "cat ucf101_tvl1_flow.zip.00* > ucf101_tvl1_flow.zip &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8gzwharYffdO",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# at /content/two-stream-action-recognition\n",
    "!rm ucf101_tvl1_flow.zip.00*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6kyhnLTDjDI2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# at /content/two-stream-action-recognition\n",
    "unzip ucf101_tvl1_flow.zip >> zip2.out &\n",
    "\n",
    "# data path has\n",
    "# u v \n",
    "\n",
    "# both u and v has format\n",
    "#v_ApplyEyeMakeup_g01_c01      v_Knitting_g16_c03\n",
    "#v_ApplyEyeMakeup_g01_c02      v_Knitting_g16_c04\n",
    "#v_ApplyEyeMakeup_g01_c03      v_Knitting_g16_c05\n",
    "#v_ApplyEyeMakeup_g01_c04      v_Knitting_g17_c01\n",
    "\n",
    "# each video folder has this format\n",
    "#frame000001.jpg  frame000047.jpg  frame000093.jpg  frame000139.jpg\n",
    "#frame000002.jpg  frame000048.jpg  frame000094.jpg  frame000140.jpg\n",
    "#frame000003.jpg  frame000049.jpg  frame000095.jpg  frame000141.jpg\n",
    "#frame000004.jpg  frame000050.jpg  frame000096.jpg  frame000142.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jUKHX3uG2_XV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# at /content/two-stream-action-recognition\n",
    "!rm ucf101_tvl1_flow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5e5-jUXqsSKZ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# view the data tvl1_flow should be 31G\n",
    "!du -sh --human-readable *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-xQMKXpIFkm",
    "colab_type": "text"
   },
   "source": [
    "# Download sample to your local machine to view the images\n",
    "\n",
    "Download to your local machine : RGB frames of v_ApplyEyeMakeup_g01_c01\n",
    "\n",
    "Download to your local machine : u optical flow frames of v_ApplyEyeMakeup_g01_c01\n",
    "\n",
    "Download to your local machine : v optical flow frames of v_ApplyEyeMakeup_g01_c01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5P2wX9D6D8GL",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!ls /content/two-stream-action-recognition/jpegs_256/v_ApplyEyeMakeup_g01_c01\n",
    "!ls /content/two-stream-action-recognition/tvl1_flow/u/v_ApplyEyeMakeup_g01_c01\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "from shutil import copyfile\n",
    "root = \"/content/two-stream-action-recognition/tvl1_flow/v/v_ApplyEyeMakeup_g01_c01\"\n",
    "\n",
    "for index,frame in enumerate(os.listdir(root)):\n",
    "  copyfile(os.path.join(root,frame), \"/tmp/tvl1_flow_v_{}\".format(index))\n",
    "  files.download(\"/tmp/tvl1_flow_v_{}\".format(index))\n",
    "\n",
    "root = \"/content/two-stream-action-recognition/tvl1_flow/u/v_ApplyEyeMakeup_g01_c01\"\n",
    "\n",
    "for index,frame in enumerate(os.listdir(root)):\n",
    "  copyfile(os.path.join(root,frame), \"/tmp/tvl1_flow_u_{}\".format(index))\n",
    "  files.download(\"/tmp/tvl1_flow_u_{}\".format(index))\n",
    "  \n",
    "root = \"/content/two-stream-action-recognition/jpegs_256/v_ApplyEyeMakeup_g01_c01\"\n",
    "\n",
    "for index,frame in enumerate(os.listdir(root)):\n",
    "  copyfile(os.path.join(root,frame), \"/tmp/jpegs_256_{}\".format(index))\n",
    "  files.download(\"/tmp/jpegs_256_{}\".format(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITynzmiq422y",
    "colab_type": "text"
   },
   "source": [
    "#Spatial Stream Model\n",
    "\n",
    "Here we train a spatial stream model.\n",
    "\n",
    "I also update the code here for everyrun to try different things\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7396AZC49S7s",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/content/two-stream-action-recognition\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "u7v9_bXQO2Dt",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# here you need your own version of the code wih your secret credentials files you created at the first cell\n",
    "\n",
    "# this is uploaded with upload.sh script using transfer.sh with cURL\n",
    "!wget -O upload.zip <LINK> > /x.txt # keep this link secret, it contains your credentials files\n",
    "!unzip -o upload.zip > /x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9tq44XXIaVql",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!python spatial_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTvFhGUq3NJZ",
    "colab_type": "text"
   },
   "source": [
    "# Motion Model Running\n",
    "\n",
    "Here we train a motion stream model.\n",
    "\n",
    "I also update the code here for every run to try different things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "t0Rw_FKB3koD",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/content/two-stream-action-recognition\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "z7_8HjqX3ocC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# here you need your own version of the code wih your secret credentials files you created at the first cell\n",
    "\n",
    "# this is uploaded with upload.sh script using transfer.sh with cURL\n",
    "!wget -O upload.zip <LINK> > /x.txt # keep this link secret, it contains your credentials files\n",
    "!unzip -o upload.zip > /x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "drkLfF0Dq2M5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!python motion_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLjNOgmdmxDy",
    "colab_type": "text"
   },
   "source": [
    "# Evaluate Streams\n",
    "evaluating streams using the data and downloading predictions pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4As6R7xmm8k3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/content/two-stream-action-recognition\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "gvFK4TKrnBbz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# here you need your own version of the code wih your secret credentials files you created at the first cell\n",
    "\n",
    "# this is uploaded with upload.sh script using transfer.sh with cURL\n",
    "!wget -O upload.zip <LINK> > /x.txt # keep this link secret, it contains your credentials files\n",
    "!unzip -o upload.zip > /x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_streams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate from pickle\n",
    "from average_fusion_demo import eval_pickles\n",
    "\n",
    "print(\"Evaluating spatial stream from pickle\")\n",
    "eval_pickles(\"./spatial.preds\", [1])  # weight list\n",
    "\n",
    "print(\"Evaluating motion stream from pickle\")\n",
    "eval_pickles(\"./motion.preds\", [1])  # weight list\n",
    "\n",
    "print(\"Average fusion from pickles\")\n",
    "eval_pickles([\"./spatial.preds\", \"./motion.preds\"], [1] * 2)  # weight list for each stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "waPFgIJCnDHU",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# download it locally\n",
    "from google.colab import files\n",
    "\n",
    "files.download('spatial.preds')  # download original spatial evaluation predictions on test data (you can evaluate locally now)\n",
    "files.download('motion.preds')  # download original motion evaluation predictions on test data (you can evaluate locally now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd7h7C5dtM4g",
    "colab_type": "text"
   },
   "source": [
    "# Generating :visual features from pre trained networks\n",
    "\n",
    "It was an experiment to try to generate the visual features of every stream and then train a separate network that fuses them..\n",
    "\n",
    "**Here's the generation part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qYHEmLrD2N-O",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/content/two-stream-action-recognition\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "F8WFDLRG2Q-Y",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# here you need your own version of the code wih your secret credentials files you created at the first cell\n",
    "\n",
    "# this is uploaded with upload.sh script using transfer.sh with cURL\n",
    "!wget -O upload.zip <LINK> > /x.txt # keep this link secret, it contains your credentials files\n",
    "!unzip -o upload.zip > /x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "un55CArcuyQY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# generate visual features\n",
    "\n",
    "!python generate_spatial_feature_dataset.py\n",
    "!python generate_motion_feature_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZKnPCe42zwa",
    "colab_type": "text"
   },
   "source": [
    "# Training :visual features from pretrained networks\n",
    "It was an experiment to try to generate the visual features of every stream and then train a separate network that fuses them..\n",
    "\n",
    "**Here's the training part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "riL0OpNM3CGE",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/content/two-stream-action-recognition\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wLhQGnLr3ERd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# here you need your own version of the code wih your secret credentials files you created at the first cell\n",
    "\n",
    "# this is uploaded with upload.sh script using transfer.sh with cURL\n",
    "!wget -O upload.zip <LINK> > /x.txt # keep this link secret, it contains your credentials files\n",
    "!unzip -o upload.zip > /x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wQrxed5_VMnz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!python recurrent_fusion_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNJ1YbKmRYmG",
    "colab_type": "text"
   },
   "source": [
    "# Downloading Original data as videos from UCF authors \n",
    "This's the original data of UCF101 (not processed frames.. they are the actual videos)\n",
    "I like the processed frames so I don't need to run optical flow algorithm which is resource-intensive\n",
    "\n",
    "You can train networks the works on videos themselves and share the results !\n",
    "(RNNS / 3D convs ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "btNko4NQCtad",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!wget \"http://crcv.ucf.edu/data/UCF101/UCF101.rar\"\n",
    "!apt install unrar\n",
    "!unrar x UCF101.rar >> out.txt\n",
    "!rm out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ehPTDhNG8ARI",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!ls UCF-101\n",
    "#each folder is just avi files\n",
    "!ls UCF-101/Knitting/v_Knitting_g17_c04.avi\n",
    "\n",
    "!echo \"===\"\n",
    "!ls UCF-101/HandstandPushups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "2gb-9sC3p9rV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "#files.download('UCF-101/Knitting/v_Knitting_g17_c04.avi')\n",
    "\n",
    "import cv2\n",
    "vidcap = cv2.VideoCapture('UCF-101/Knitting/v_Knitting_g17_c04.avi')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "  success,image = vidcap.read()\n",
    "  #print('Read a new frame: ', success)\n",
    "  count += 1\n",
    "  \n",
    "print(count) # 182 also in dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI5FId4CZB8q",
    "colab_type": "text"
   },
   "source": [
    "# Flownet(Failed experiment)(Left as future work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "OzTJmCd0ZDjJ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sampepose/flownet2-tf.git\n",
    "!pip install pypng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "W6vb_ModdznK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!ls  flownet2-tf/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "hJjR0BFreEzX",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"flownet2-tf/checkpoints/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bZc2q5CieJ1e",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!sh download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "YJME7KWCd5BV",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6FwCY1--vQA8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!apt update -qq;\n",
    "!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
    "!dpkg -i cuda-repo-ubuntu1604–8–0-local-ga2_8.0.61–1_amd64-deb 2> /dev/null;\n",
    "!apt-key add /var/cuda-repo-8–0-local-ga2/7fa2af80.pub;\n",
    "!apt-get update -qq;\n",
    "!apt-get install cuda gcc-5 g++-5 -y -qq;\n",
    "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
    "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
    "!apt install cuda-8.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6h1ZwCx2uFbT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "g6jlCn2jgb8F",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!cat src/flownet2/test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYyCqL_uVF_N",
    "colab_type": "text"
   },
   "source": [
    "#Motion net(Failed experiment)(Left as future work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "gA-NDc7bVJ8U",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!apt install -y caffe-cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9OQ-CFP3Aw8n",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/bryanyzhu/Hidden-Two-Stream.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mfG_QtVGA3-h",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!ls Hidden-Two-Stream/models/multiframe_MotionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QU2kvueCNCJp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# this code only runs on colab <<< \n",
    "# step 1 get some token (do this on drive)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from google.colab import files\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "V1ulI159NMzy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Download a file based on its file ID.\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "file_id = '0B-bJpXHBmFWDVU5DRTY4Ym02TFE'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('MotionNet.caffemodel') # Download file as 'catlove.png' and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UG5uSLtnUI1L",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ethereon/caffe-tensorflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "z-NwegYtMezM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!ls Hidden-Two-Stream/models/multiframe_MotionNet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "yNEGdPA_URDN",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!./caffe-tensorflow/convert.py Hidden-Two-Stream/models/multiframe_MotionNet/deploy.prototxt --code-output-path=deploy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vf5ragl-UrM2",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!./caffe-tensorflow/convert.py Hidden-Two-Stream/models/multiframe_MotionNet/train.prototxt --code-output-path=train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Erk1nZSEBT4a",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!cat Hidden-Two-Stream/models/multiframe_MotionNet/deploy.prototxt "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "download.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [
    "LYRff_wiRRlF",
    "0zcjJG-MjwRw",
    "27f8QtTK6aDg",
    "lRGtrOssijlN",
    "M-xQMKXpIFkm",
    "ITynzmiq422y",
    "zd7h7C5dtM4g",
    "NZKnPCe42zwa",
    "UNJ1YbKmRYmG",
    "lI5FId4CZB8q",
    "jYyCqL_uVF_N"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
