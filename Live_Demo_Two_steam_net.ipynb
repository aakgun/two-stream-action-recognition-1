{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYRff_wiRRlF"
   },
   "source": [
    "# This is a live demo of video action recognition using two-stream architecture\n",
    "\n",
    "\n",
    "This will clone my repo and download the models on drive and uses them to infer the output in a live frame-level demo. then an output video will be generated showing the output prediction for each frame accordingly.\n",
    "\n",
    "I suggest running all the cells and have a 5 mins-break then view the output video :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZF7ijdVCBUF"
   },
   "source": [
    "# Starting by installation process \n",
    "\n",
    "\n",
    "This will clone my repo and download the models on drive.\n",
    "I used gdown.pl tool for downloading my public checkpoints on drive with no authentication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1584
    },
    "colab_type": "code",
    "id": "JBvzBNJMRVZM",
    "outputId": "07ec1397-a14f-48d8-b881-c6346240cf7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'two-stream-action-recognition' already exists and is not an empty directory.\n",
      "fatal: destination path 'gdown.pl' already exists and is not an empty directory.\n",
      "Cannot open cookies file ‘gdown.cookie.temp’: No such file or directory\n",
      "--2019-07-27 22:03:41--  https://docs.google.com/uc?id=1djGzpxAYFvNX-UaQ7ONqDHGgnzc8clBK&export=download\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.14.238, 2607:f8b0:400a:808::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.14.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘spatial.zip’\n",
      "\n",
      "     0K                                     37.7M=0s\n",
      "\n",
      "2019-07-27 22:03:41 (37.7 MB/s) - ‘spatial.zip’ saved [3254]\n",
      "\n",
      "--2019-07-27 22:03:41--  https://docs.google.com/uc?export=download&confirm=g4jR&id=1djGzpxAYFvNX-UaQ7ONqDHGgnzc8clBK\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.14.238, 2607:f8b0:400a:808::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.14.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-10-50-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2a00rkrd9177fhr7hdls3er7q7ttqdgr/1564286400000/13087578489450988105/*/1djGzpxAYFvNX-UaQ7ONqDHGgnzc8clBK?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-07-27 22:03:42--  https://doc-10-50-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2a00rkrd9177fhr7hdls3er7q7ttqdgr/1564286400000/13087578489450988105/*/1djGzpxAYFvNX-UaQ7ONqDHGgnzc8clBK?e=download\n",
      "Resolving doc-10-50-docs.googleusercontent.com (doc-10-50-docs.googleusercontent.com)... 172.217.3.161, 2607:f8b0:400a:804::2001\n",
      "Connecting to doc-10-50-docs.googleusercontent.com (doc-10-50-docs.googleusercontent.com)|172.217.3.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘spatial.zip’\n",
      "\n",
      "     0K ........ ........ ........ ........ 10.5M\n",
      " 32768K ........ ........ ........ ........ 10.5M\n",
      " 65536K ........ ........ ........ ........ 10.4M\n",
      " 98304K ........ ........ ........ ........ 10.8M\n",
      "131072K ........ ........ ........ ........ 9.73M\n",
      "163840K ........ ........ ........ ........ 10.8M\n",
      "196608K ........ ........ ........ .......  10.6M=21s\n",
      "\n",
      "2019-07-27 22:04:03 (10.4 MB/s) - ‘spatial.zip’ saved [234458427]\n",
      "\n",
      "Cannot open cookies file ‘gdown.cookie.temp’: No such file or directory\n",
      "--2019-07-27 22:04:04--  https://docs.google.com/uc?id=1kvslNL8zmZYaHRmhgAM6-l_pNDDA0EKZ&export=download\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.14.238, 2607:f8b0:400a:808::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.14.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘motion.zip’\n",
      "\n",
      "     0K                                     19.7M=0s\n",
      "\n",
      "2019-07-27 22:04:04 (19.7 MB/s) - ‘motion.zip’ saved [3254]\n",
      "\n",
      "--2019-07-27 22:04:04--  https://docs.google.com/uc?export=download&confirm=4eij&id=1kvslNL8zmZYaHRmhgAM6-l_pNDDA0EKZ\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.14.238, 2607:f8b0:400a:808::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.14.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0c-2g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/18vghi0p4nfsflutm1ss5tbm401s7v6n/1564286400000/09175336730828036060/*/1kvslNL8zmZYaHRmhgAM6-l_pNDDA0EKZ?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-07-27 22:04:04--  https://doc-0c-2g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/18vghi0p4nfsflutm1ss5tbm401s7v6n/1564286400000/09175336730828036060/*/1kvslNL8zmZYaHRmhgAM6-l_pNDDA0EKZ?e=download\n",
      "Resolving doc-0c-2g-docs.googleusercontent.com (doc-0c-2g-docs.googleusercontent.com)... 172.217.3.161, 2607:f8b0:400a:804::2001\n",
      "Connecting to doc-0c-2g-docs.googleusercontent.com (doc-0c-2g-docs.googleusercontent.com)|172.217.3.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘motion.zip’\n",
      "\n",
      "     0K ........ ........ ........ ........ 11.0M\n",
      " 32768K ........ ........ ........ ........ 11.2M\n",
      " 65536K ........ ........ ........ ........ 11.2M\n",
      " 98304K ........ ........ ........ ........ 10.4M\n",
      "131072K ........ ........ ........ ........ 11.2M\n",
      "163840K ........ ........ ........ ........ 11.2M\n",
      "196608K ........ ........ ........ .......  10.7M=20s\n",
      "\n",
      "2019-07-27 22:04:25 (11.0 MB/s) - ‘motion.zip’ saved [234618151]\n",
      "\n",
      "Archive:  spatial.zip\n",
      "replace spatial.log? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Archive:  motion.zip\n",
      "replace motion.log? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "^C\n",
      "^C\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforkpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-218f2244e348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U -q PyDrive 2> s.txt >> s.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install opencv-python 2> s.txt >> s.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install imgaug 2> s.txt >> s.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scikit-video 2> s.txt >> s.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2450\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2452\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!git clone https://github.com/mohammed-elkomy/two-stream-action-recognition.git\n",
    "os.chdir(\"two-stream-action-recognition\")\n",
    "\n",
    "!git clone https://github.com/circulosmeos/gdown.pl.git\n",
    "!./gdown.pl/gdown.pl https://drive.google.com/file/d/1djGzpxAYFvNX-UaQ7ONqDHGgnzc8clBK/view \"spatial.zip\" \n",
    "!./gdown.pl/gdown.pl https://drive.google.com/file/d/1kvslNL8zmZYaHRmhgAM6-l_pNDDA0EKZ/view \"motion.zip\"\n",
    "!unzip spatial.zip\n",
    "!unzip motion.zip\n",
    "\n",
    "!pip install -U -q PyDrive 2> s.txt >> s.txt\n",
    "!pip install opencv-python 2> s.txt >> s.txt\n",
    "!pip install imgaug 2> s.txt >> s.txt\n",
    "!pip install scikit-video 2> s.txt >> s.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imgaug\n",
    "pip install tensorflow-gpu\n",
    "sudo pip install scikit-video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ak9la5QJCk5U"
   },
   "source": [
    "# Showing the demo\n",
    "\n",
    "\n",
    "It will start by selecting one random video of the videos **testing video samples** stored in my repo(it contains 100 videos from the test dataset and you can add more, originally it was possible to get a sinlge video by name using an HTTP request but the UCF101 changed their site a little bit  and it's not possible now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"two-stream-action-recognition\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inc9wGsPXsWU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Action Recognition Walkthrough.ipynb'\t motion.preds\n",
      " LICENSE\t\t\t\t motion.zip\n",
      " Live_Demo_Two_steam_net.ipynb\t\t motion_trainer.py\n",
      " UCF_list\t\t\t\t readme.md\n",
      " average_fusion_demo.py\t\t\t recurrent_fusion_trainer.py\n",
      " configs\t\t\t\t spatial.h5\n",
      " evaluate_streams.py\t\t\t spatial.log\n",
      " evaluation\t\t\t\t spatial.preds\n",
      " frame_dataloader\t\t\t spatial.zip\n",
      " generate_motion_feature_dataset.py\t spatial_trainer.py\n",
      " generate_spatial_feature_dataset.py\t'testing video samples'\n",
      " models\t\t\t\t\t twostream.py\n",
      " motion.h5\t\t\t\t upload.sh\n",
      " motion.log\t\t\t\t utils\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "high_resolution_video = True # for good internet :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBgtoujkDaMV"
   },
   "outputs": [],
   "source": [
    "# importing those only once\n",
    "import os\n",
    "import cv2\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from evaluation import legacy_load_model\n",
    "from evaluation.evaluation import *\n",
    "\n",
    "import random\n",
    "from frame_dataloader import DataUtil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import skvideo.io\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMwBhP58Dd-m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Function `Scale()` is deprecated. Use `Resize` instead. Resize has the exactly same interface as Scale.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# defining functions and global objects\n",
    "\n",
    "# dictionary of class names\n",
    "data_util = DataUtil(path= './UCF_list/', split=\"01\")\n",
    "action_names =  {v:k for k,v in data_util.action_to_label.items()} # class name dictionary\n",
    "\n",
    "stacked_frames = 10\n",
    "\n",
    "# image resize augmenter to be fed into the network\n",
    "augmenter = iaa.Sequential([\n",
    "    iaa.Scale({\"height\": 299, \"width\": 299})\n",
    "])\n",
    "\n",
    "\n",
    "def convert_to_image(flow_image):\n",
    "    \"\"\"\n",
    "    this is the conversion function of each flow frame\n",
    "    based on the cpp version of extracting optical flow https://github.com/feichtenhofer/gpu_flow/blob/master/compute_flow.cpp\n",
    "    \"\"\"\n",
    "    l, h = -20, 20\n",
    "    return (255 * (flow_image - l) / (h - l)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def stack_opticalflow(start_frame_index, stacked_frames):  # returns numpy (h,w,stacked*2) = one sample\n",
    "    \"\"\"\n",
    "    Stacks \"stacked_frames\" u/v frames on a single numpy array : (h,w,stacked*2)\n",
    "    \"\"\"\n",
    "    first_optical_frame_u = original_u_frames[start_frame_index]  # horizontal\n",
    "    first_optical_frame_v = original_v_frames[start_frame_index]  # vertical\n",
    "\n",
    "    stacked_optical_flow_sample = np.zeros(first_optical_frame_u.shape + (2 * stacked_frames,), dtype=np.uint8)  # with channel dimension of  stacked_frames(u)+ stacked_frames(v)\n",
    "\n",
    "    stacked_optical_flow_sample[:, :, 0] = first_optical_frame_u\n",
    "    stacked_optical_flow_sample[:, :, 0 + stacked_frames] = first_optical_frame_v\n",
    "\n",
    "    for index, optical_frame_id in enumerate(range(start_frame_index + 1, start_frame_index + stacked_frames), 1):  # index starts at 1 placed after the first one\n",
    "        stacked_optical_flow_sample[:, :, index] = original_u_frames[optical_frame_id]\n",
    "        stacked_optical_flow_sample[:, :, index + stacked_frames] = original_v_frames[optical_frame_id]\n",
    "\n",
    "    return stacked_optical_flow_sample\n",
    "\n",
    "\n",
    "def get_image_from_fig(fig):\n",
    "    \"\"\"\n",
    "    converts matplotlib figure into a numpy array for demo video generation\n",
    "    \"\"\"\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZw28kKGEEaV"
   },
   "source": [
    "### loading keras models just downloaded from drive (loaded once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "IC9Dgq_YDd9j",
    "outputId": "0acaf201-0f5b-4000-b288-36075599f0cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 12:07:41.249042 547849293840 deprecation.py:506] From /home/odroid/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0803 12:07:41.254381 547849293840 deprecation.py:506] From /home/odroid/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0803 12:07:41.269337 547849293840 deprecation.py:506] From /home/odroid/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Spatial stream\n",
      "Tensor(\"predictions_target:0\", shape=(?, ?), dtype=float32) Tensor(\"predictions/Softmax:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"predictions_target:0\", shape=(?, ?), dtype=float32) Tensor(\"predictions/Softmax:0\", shape=(?, 101), dtype=float32)\n",
      "Model: \"spatial_xception\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_spatial (InputLayer)   [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "data_norm (BatchNormalizatio (None, 299, 299, 3)       6         \n",
      "_________________________________________________________________\n",
      "xception (Model)             (None, 10, 10, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "avg_pool (GlobalAveragePooli (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 101)               206949    \n",
      "=================================================================\n",
      "Total params: 21,068,435\n",
      "Trainable params: 21,013,901\n",
      "Non-trainable params: 54,534\n",
      "_________________________________________________________________\n",
      "Loading Motion stream\n",
      "Tensor(\"xception_target:0\", shape=(?, ?), dtype=float32) Tensor(\"xception_1/predictions/Softmax:0\", shape=(?, 101), dtype=float32)\n",
      "Tensor(\"xception_target:0\", shape=(?, ?), dtype=float32) Tensor(\"xception_1/predictions/Softmax:0\", shape=(?, 101), dtype=float32)\n",
      "Model: \"motion_xception\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_motion (InputLayer)    [(None, 299, 299, 20)]    0         \n",
      "_________________________________________________________________\n",
      "data_norm (BatchNormalizatio (None, 299, 299, 20)      40        \n",
      "_________________________________________________________________\n",
      "xception (Model)             (None, 101)               21073325  \n",
      "=================================================================\n",
      "Total params: 21,073,365\n",
      "Trainable params: 21,018,797\n",
      "Non-trainable params: 54,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# legacy_load_model is an older version of keras load_model since keras API changed a little bit when I was working on action recognition \n",
    "\n",
    "# load into ram\n",
    "print(\"Loading Spatial stream\")\n",
    "spatial_model_restored = legacy_load_model(filepath=\"spatial.h5\", custom_objects={'sparse_categorical_cross_entropy_loss': sparse_categorical_cross_entropy_loss, \"acc_top_1\": acc_top_1, \"acc_top_5\": acc_top_5})\n",
    "spatial_model_restored.summary()\n",
    "\n",
    "\n",
    "# load into ram\n",
    "print(\"Loading Motion stream\")\n",
    "motion_model_restored = legacy_load_model(filepath=\"motion.h5\", custom_objects={'sparse_categorical_cross_entropy_loss': sparse_categorical_cross_entropy_loss, \"acc_top_1\": acc_top_1, \"acc_top_5\": acc_top_5})\n",
    "motion_model_restored.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj50AswwEOYC"
   },
   "source": [
    "### Selecting one video and process it for RGB frames and optical flow frames\n",
    "optical flow frames are computed using TVL1 which is never real time on CPU, might take few minutes for long vidoes (I process them on CPU since GPU version requires building opencv from the source and doing some nasty things not helpful for a short demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Action Recognition Walkthrough.ipynb'\t motion_trainer.py\n",
      " average_fusion_demo.py\t\t\t motion.zip\n",
      " configs\t\t\t\t __pycache__\n",
      " demo.mp4\t\t\t\t readme.md\n",
      " evaluate_streams.py\t\t\t recurrent_fusion_trainer.py\n",
      " evaluation\t\t\t\t spatial.h5\n",
      " frame_dataloader\t\t\t spatial.log\n",
      " gdown.pl\t\t\t\t spatial.preds\n",
      " generate_motion_feature_dataset.py\t spatial_trainer.py\n",
      " generate_spatial_feature_dataset.py\t spatial.zip\n",
      " LICENSE\t\t\t\t s.txt\n",
      " Live_Demo_Two_steam_net.ipynb\t\t'testing video samples'\n",
      " models\t\t\t\t\t UCF_list\n",
      " motion.h5\t\t\t\t upload.sh\n",
      " motion.log\t\t\t\t utils\n",
      " motion.preds\t\t\t\t v_HammerThrow_g10_c03.avi\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'two-stream-action-recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-15d7dda8d494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"two-stream-action-recognition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'two-stream-action-recognition'"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "os.chdir(\"two-stream-action-recognition\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bOgm4dL0Ezje",
    "outputId": "1bbe26fb-52ed-4664-e14b-85583ba9f0e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n",
      "testing video samples/v_BabyCrawling_g19_c01.avi\n",
      "selected_video: testing video samples/v_BabyCrawling_g19_c01.avi\n",
      "frame rate for demo: 25.0\n"
     ]
    }
   ],
   "source": [
    "# select a random video\n",
    "from cv2 import __version__\n",
    "print(__version__)\n",
    "\n",
    "#video_name = random.choice(os.listdir(\"testing video samples\"))\n",
    "video_name = 'v_BabyCrawling_g19_c01.avi'\n",
    "print(os.path.join(\"testing video samples\",video_name))\n",
    "selected_video=os.path.join(\"testing video samples\",video_name)\n",
    "#selected_video=\"v_HammerThrow_g10_c03.avi\"\n",
    "print(\"selected_video:\",selected_video)\n",
    "\n",
    "vidcap = cv2.VideoCapture(selected_video)\n",
    "print(\"frame rate for demo:\",vidcap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "success, image = vidcap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames count in video 94\n"
     ]
    }
   ],
   "source": [
    "# make the rgb frames\n",
    "original_rgb_frames = []\n",
    "\n",
    "while success:\n",
    "    original_rgb_frames.append(image)\n",
    "    success, image = vidcap.read()\n",
    "\n",
    "print(\"frames count in video\", len(original_rgb_frames))\n",
    "\n",
    "# make the optical flow frames\n",
    "original_v_frames = []\n",
    "original_u_frames = []\n",
    "\n",
    "frames = list(map(lambda frame: cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0, original_rgb_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "KzBVSjDiChk1",
    "outputId": "29d5b45e-e15a-4bd9-c0e0-28082d06be73"
   },
   "outputs": [],
   "source": [
    "#for opencv 4\n",
    "optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "#for opencv 3\n",
    "#optical_flow = cv2.DualTVL1OpticalFlow_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing tvl flow of frame  0\n",
      "processing tvl flow of frame  10\n",
      "processing tvl flow of frame  20\n",
      "processing tvl flow of frame  30\n",
      "processing tvl flow of frame  40\n",
      "processing tvl flow of frame  50\n",
      "processing tvl flow of frame  60\n",
      "processing tvl flow of frame  70\n",
      "processing tvl flow of frame  80\n",
      "processing tvl flow of frame  90\n",
      "original_rgb_frames: 94 original_u_frames: 93 original_v_frames: 93\n"
     ]
    }
   ],
   "source": [
    "for frame_index in range(len(frames) - 1):\n",
    "    if frame_index % 10 == 0:\n",
    "        print(\"processing tvl flow of frame \",frame_index)\n",
    "\n",
    "    flow = optical_flow.calc(frames[frame_index], frames[frame_index + 1], None)\n",
    "    u_frame = convert_to_image(flow[..., 0])\n",
    "    v_frame = convert_to_image(flow[..., 1])\n",
    "\n",
    "    original_v_frames.append(v_frame)\n",
    "    original_u_frames.append(u_frame)\n",
    "\n",
    "print(\"original_rgb_frames:\", len(original_rgb_frames), \"original_u_frames:\", len(original_u_frames), \"original_v_frames:\", len(original_v_frames))\n",
    "\n",
    "# generate spatial batch as done in the dataloader\n",
    "spatial_batch = []\n",
    "for image in original_rgb_frames:\n",
    "    spatial_batch.append(\n",
    "        cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    )\n",
    "\n",
    "spatial_batch = np.array(augmenter.augment_images(spatial_batch), dtype=np.float32) / 255.0\n",
    "\n",
    "# generate motion batch as done in the dataloader\n",
    "motion_batch = []\n",
    "\n",
    "for first_optical_frame_id in range(len(original_u_frames) - stacked_frames):\n",
    "    motion_batch.append(  # append one sample which is (h,w,stacked*2)\n",
    "        stack_opticalflow(start_frame_index=first_optical_frame_id, stacked_frames=stacked_frames)\n",
    "    )\n",
    "motion_batch = np.array(augmenter.augment_images(motion_batch), dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tEZrTZkE6HG"
   },
   "source": [
    "### Predict the output of each frame organized in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVVWC5CWChjx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> spatial <<<<<\n",
      "start time:  1564891094.8041284\n",
      "end time:  1564891239.5454874\n",
      "total time:  144.74135899543762\n",
      ">>>>> motion <<<<<\n",
      "start time:  1564891239.5616086\n",
      "end time:  1564891855.5532086\n",
      "total time:  615.9916000366211\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\"\"\"\n",
    "predict spatial stream output\n",
    "\"\"\"\n",
    "print(\">>>>> spatial <<<<<\")\n",
    "startTime = time.time()\n",
    "print(\"start time: \", startTime)\n",
    "spatial_pred = spatial_model_restored.predict(spatial_batch)\n",
    "endTime= time.time()\n",
    "print(\"end time: \", endTime)\n",
    "print(\"total time: \", endTime - startTime)\n",
    "spatial_classes = np.argsort(spatial_pred,axis=1)[:,:-6:-1]\n",
    "spatial_scores = np.sort(spatial_pred,axis=1)[:,:-6:-1]\n",
    "\n",
    "\"\"\"\n",
    "predict motion stream output\n",
    "\"\"\"\n",
    "print(\">>>>> motion <<<<<\")\n",
    "startTime = time.time()\n",
    "print(\"start time: \", startTime)\n",
    "motion_pred = motion_model_restored.predict(motion_batch)\n",
    "endTime= time.time()\n",
    "print(\"end time: \", endTime)\n",
    "print(\"total time: \", endTime - startTime)\n",
    "motion_classes = np.argsort(motion_pred,axis=1)[:,:-6:-1]\n",
    "motion_scores = np.sort(motion_pred,axis=1)[:,:-6:-1]\n",
    "\"\"\"\n",
    "get the average output prediction\n",
    "\"\"\"\n",
    "average_pred = motion_pred + spatial_pred[:motion_pred.shape[0],]\n",
    "average_classes = np.argsort(average_pred,axis=1)[:,:-6:-1]\n",
    "average_scores = np.sort(average_pred,axis=1)[:,:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU_7S5aoMEBB"
   },
   "outputs": [],
   "source": [
    "def make_bar_chart(classes,scores):\n",
    "    height = scores.tolist()\n",
    "    bars = [action_names[class_index] for class_index in classes]\n",
    "    y_pos = np.arange(len(bars))\n",
    "    \n",
    "    bar = plt.bar(y_pos, height, color=['yellow', 'red', 'green', 'blue', 'cyan'])\n",
    "    # plt.xticks(y_pos, bars, rotation=90) this will draw them below\n",
    "    # plt.tick_params(axis=\"x\",labelsize=10,direction=\"in\", pad=-15)\n",
    "    plt.ylim(top=1)  \n",
    "    plt.ylim(bottom=0) \n",
    "    \n",
    "    for bar_id,rect in enumerate(bar):\n",
    "        plt.text(rect.get_x() + rect.get_width()/2.0, .5, bars[bar_id], ha='center', va='center', rotation=75,fontdict={'fontsize': 13 if high_resolution_video else 10})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "R5_T5lc1HMl-",
    "outputId": "967f069a-7a71-4476-af06-817a7bd1c7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing frame  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing frame  10\n",
      "processing frame  20\n",
      "processing frame  30\n",
      "processing frame  40\n",
      "processing frame  50\n",
      "processing frame  60\n",
      "processing frame  70\n",
      "processing frame  80\n"
     ]
    }
   ],
   "source": [
    "# Define the codec and create VideoWriter object.The output is stored in 'demo.mp4' file.\n",
    "\n",
    "writer = skvideo.io.FFmpegWriter(\"demo.mp4\", inputdict={\n",
    "      '-r': '16',\n",
    "    })\n",
    "\n",
    "gs = gridspec.GridSpec(2, 3,\n",
    "                       width_ratios=[1, 1,1],\n",
    "                       height_ratios=[1.5, 1]\n",
    "                       )\n",
    "\n",
    "gs.update(wspace=0.2,hspace=0)\n",
    "\n",
    "# generating output video\n",
    "for frame_index in range(motion_classes.shape[0]): \n",
    "    if high_resolution_video :\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        fig.suptitle(\"Demo for {}\".format(video_name), fontsize=24)\n",
    "\n",
    "        fig.text(.125,0.91,\"Average Prediction from spatial stream: {}\".format(action_names[np.mean(spatial_pred,axis = 0).argmax()]),color='r', fontsize=18)\n",
    "        fig.text(.125,.87,\"Average Prediction from motion stream: {}\".format(action_names[np.mean(motion_pred,axis = 0).argmax()]),color='g',fontsize=18)\n",
    "        fig.text(.125,.83,\"Average Prediction from both streams: {}\".format(action_names[np.mean(average_pred,axis = 0).argmax()]),color='b', fontsize=18)\n",
    "    else :\n",
    "        fig = plt.figure(figsize=(9, 6))\n",
    "        fig.suptitle(\"Demo for {}\".format(video_name), fontsize=16)\n",
    "\n",
    "        fig.text(.125,0.91,\"Average Prediction from spatial stream: {}\".format(action_names[np.mean(spatial_pred,axis = 0).argmax()]),color='r', fontsize=13)\n",
    "        fig.text(.125,.87,\"Average Prediction from motion stream: {}\".format(action_names[np.mean(motion_pred,axis = 0).argmax()]),color='g',fontsize=13)\n",
    "        fig.text(.125,.83,\"Average Prediction from both streams: {}\".format(action_names[np.mean(average_pred,axis = 0).argmax()]),color='b', fontsize=13)\n",
    "    \n",
    "\n",
    "    if frame_index % 10 == 0:\n",
    "        print(\"processing frame \",frame_index)\n",
    "    ##########################################################\n",
    "    # rgb frame\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax.set_title(\"RGB frame\", fontsize=16 if high_resolution_video else 13)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.imshow(cv2.cvtColor(original_rgb_frames[frame_index],cv2.COLOR_RGB2BGR))\n",
    "    ##########################################################\n",
    "    # optical flow frame\n",
    "    ax = plt.subplot(gs[1])\n",
    "    ax.set_title(\"TVL1 Optical flow u-frame\", fontsize=16 if high_resolution_video else 13)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.imshow(original_u_frames[frame_index],cmap=\"inferno\") # viridis,inferno,plasma,magma\n",
    "    ##########################################################\n",
    "    # optical flow frame\n",
    "    ax = plt.subplot(gs[2])\n",
    "    ax.set_title(\"TVL1 Optical flow v-frame\", fontsize= 16 if high_resolution_video else 13)\n",
    "\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.imshow(original_v_frames[frame_index],cmap=\"inferno\") # viridis,inferno,plasma,magma\n",
    "    ##########################################################\n",
    "    # prediction scores\n",
    "    ax = plt.subplot(gs[3])\n",
    "    ax.set_title(\"Spatial Stream Output scores\",fontsize= 16 if high_resolution_video else 13)\n",
    "\n",
    "    make_bar_chart(spatial_classes[frame_index],spatial_scores[frame_index])\n",
    "    ##########################################################\n",
    "    # prediction scores\n",
    "    ax = plt.subplot(gs[4])\n",
    "    ax.set_title(\"Motion Stream Output scores\",fontsize= 16 if high_resolution_video else 13)\n",
    "\n",
    "    make_bar_chart(motion_classes[frame_index],motion_scores[frame_index])\n",
    "    ##########################################################\n",
    "    # prediction scores\n",
    "    ax = plt.subplot(gs[5])\n",
    "    ax.set_title(\"Average Output scores\",fontsize= 16 if high_resolution_video else 13)\n",
    "\n",
    "    make_bar_chart(average_classes[frame_index],average_scores[frame_index])\n",
    "    ##########################################################\n",
    "    fig.tight_layout( pad=0, h_pad=0, w_pad=0)\n",
    "    writer.writeFrame(get_image_from_fig(fig))\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "057Q-B2o2H9r",
    "outputId": "3706f3fa-5a90-4c6a-a56c-19689bc54581"
   },
   "source": [
    "#### video = io.open(\"demo.mp4\" , 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "\n",
    "HTML(data='''<video controls autoplay loop>\n",
    "\t\t\t<source type=\"video/mp4\" src=\"data:video/mp4;base64,{}\"\n",
    "      \t\t</video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Live Demo Two-steam net.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
